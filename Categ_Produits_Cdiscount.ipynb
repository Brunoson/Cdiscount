{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catégorisation des Produits Cdiscount avec [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"R\"/></a> -- Bruno ABILOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABILOU\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importation des packages génériques et ceux \n",
    "# des librairie ML et MLlib\n",
    "##Nettoyage\n",
    "import nltk\n",
    "import re\n",
    "##Liste\n",
    "from numpy import array\n",
    "##Temps\n",
    "import time\n",
    "##Row and Vector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "##Hashage et vectorisation\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "##Regression logistique\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "##Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "##Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "##Pour la création des DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd \n",
    "from pyspark import SparkContext \n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|category_id|            category|               title|         description|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|          0|TELEPHONIE - GPS ...|Coque  Samsung AC...|Coque  Samsung AC...|\n",
      "|          0|TELEPHONIE - GPS ...|Coque rigide Viol...|Coque rigide Viol...|\n",
      "|          0|TELEPHONIE - GPS ...|Coque rigide Rose...|Coque rigide Rose...|\n",
      "|          0|TELEPHONIE - GPS ...|Coque souple Gris...|Coque souple Gris...|\n",
      "|          0|TELEPHONIE - GPS ...|Coque HTC One S 4...|Coque HTC One S 4...|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création de la base distribuée\n",
    "sqlc=SQLContext(sc) \n",
    "data= pd.read_csv(\"train.csv\",sep=\",\",encoding='latin-1')\n",
    "data_all=sqlc.createDataFrame(data) \n",
    "\n",
    "# Cette ligne permet de visualiser les 5 premières lignes de la DataFrame \n",
    "data_all.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Division de la base en apprentissage et validation\n",
    "data_all= pd.read_csv(\"train.csv\",sep=\",\",encoding='latin-1')\n",
    "dataTrain, DataTest = train_test_split(data_all, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Création d'un Transformer pour l'étape de stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class MyNltkStemmer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(MyNltkStemmer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        STEMMER = nltk.stem.SnowballStemmer('french')\n",
    "        def clean_text(tokens):\n",
    "            tokens_stem = [ STEMMER.stem(token) for token in tokens]\n",
    "            return tokens_stem\n",
    "        udfCleanText =  udf(lambda lt : clean_text(lt), ArrayType(StringType()))\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udfCleanText(in_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des différentes étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ABILOU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# liste des mots à supprimer\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('french'))\n",
    "# Fonction tokenizer qui permet de remplacer un long texte par une liste de mot\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"tokenizedDescr\", \n",
    "                                pattern=\"[^a-z_]\",minTokenLength=3, gaps=True)\n",
    "\n",
    "# Fonction StopWordsRemover qui permet de supprimer des mots\n",
    "remover = StopWordsRemover(inputCol=\"tokenizedDescr\", outputCol=\"stopTokenizedDescr\", \n",
    "                           stopWords = list(STOPWORDS))\n",
    "# Stemmer \n",
    "stemmer = MyNltkStemmer(inputCol=\"stopTokenizedDescr\", outputCol=\"cleanDescr\")\n",
    "\n",
    "# Indexer\n",
    "indexer = StringIndexer(inputCol=\"categorie_id\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Hasing\n",
    "hashing_tf = HashingTF(inputCol=\"cleanDescr\", outputCol='tf', numFeatures=10000)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"tfidf\")\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01, fitIntercept=False, tol=0.0001,\n",
    "            family = \"multinomial\", elasticNetParam=0.0, featuresCol=\"tfidf\", \n",
    "                        labelCol=\"categoryIndex\") \n",
    "# Creation du pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, stemmer, indexer, hashing_tf, idf, lr ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation du pipeline et de l'erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(dataTrain)\n",
    "predictionsDF = model.transform(DataTest)\n",
    "labelsAndPredictions = predictionsDF.select(\"categoryIndex\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "testErr = 1-nb_good_prediction/n_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
